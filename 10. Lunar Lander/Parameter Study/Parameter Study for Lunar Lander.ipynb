{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "af7aaaea8c2b14deb2496c07f500f616",
     "grade": false,
     "grade_id": "cell-b37957442e1fbd8b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Parameter Study for Lunar Lander\n",
    "\n",
    "Adapted from the Reinforcement Learning specialization course on Coursera, this project involves several crucial steps: developing a realistic simulator, selecting and implementing a suitable reinforcement learning algorithm, and optimizing its hyperparameters.\n",
    "\n",
    "Having completed the implementation of the Lunar Lander environment and the agent using neural networks and the Adam optimizer, attention now turns to understanding how different meta-parameters affect agent performance. Key meta-parameters include step-size, the temperature parameter for the softmax policy, and the replay buffer capacity. While rules of thumb can guide initial choices, a detailed analysis of these parameters’ impact on performance provides deeper insights.\n",
    "\n",
    "In this notebook, the focus will be on analyzing agent performance across various step-size parameters through careful experimentation.\n",
    "\n",
    "**Objectives:**\n",
    "\n",
    "- Develop a script to run the agent and environment with a range of parameter values to assess performance.\n",
    "- Analyze the effect of the step-size parameter on agent performance by examining its sensitivity curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "619bb4ae4c61f3c5d28bab13a2deaea5",
     "grade": false,
     "grade_id": "cell-9b62cd317dfd157f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Packages\n",
    "\n",
    "- [numpy](www.numpy.org) : Fundamental package for scientific computing with Python.\n",
    "- [matplotlib](http://matplotlib.org) : Library for plotting graphs in Python.\n",
    "- [RL-Glue](http://www.jmlr.org/papers/v10/tanner09a.html) : Library for reinforcement learning experiments.\n",
    "- [tqdm](https://tqdm.github.io/) : A package to display progress bar when running experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the following line to install the packages.\n",
    "# !pip install numpy matplotlib tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7bfa3a156b7eaccdbd19dc9bdad0efc3",
     "grade": false,
     "grade_id": "cell-b48b54531e30224f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "from rl_glue import RLGlue\n",
    "from environment import BaseEnvironment\n",
    "from agent import BaseAgent\n",
    "from dummy_environment import DummyEnvironment\n",
    "from dummy_agent import DummyAgent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "29e149431bdee8567eaa8b98232f4e14",
     "grade": false,
     "grade_id": "cell-f8442d4f392c0918",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## 1. Parameter Study Script\n",
    "\n",
    "This section involves writing a script to conduct parameter studies. The task is to implement the `run_experiment()` function, which will take an environment and an agent to perform a parameter study on both the step-size and temperature parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "7aeae683dd5fd3bcfc7fa066b233a974",
     "grade": false,
     "grade_id": "cell-2a279281a516eb2c",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def run_experiment(environment, agent, environment_parameters, agent_parameters, experiment_parameters):\n",
    "    \n",
    "    \"\"\"\n",
    "    Assume environment_parameters dict contains:\n",
    "    {\n",
    "        input_dim: integer,\n",
    "        num_actions: integer,\n",
    "        discount_factor: float\n",
    "    }\n",
    "    \n",
    "    Assume agent_parameters dict contains:\n",
    "    {\n",
    "        step_size: 1D numpy array of floats,\n",
    "        tau: 1D numpy array of floats\n",
    "    }\n",
    "    \n",
    "    Assume experiment_parameters dict contains:\n",
    "    {\n",
    "        num_runs: integer,\n",
    "        num_episodes: integer\n",
    "    }    \n",
    "    \"\"\"\n",
    "    \n",
    "    ### Instantiate rl_glue from RLGlue    \n",
    "    rl_glue = RLGlue(environment, agent)\n",
    "\n",
    "    os.system('sleep 1') # to prevent tqdm printing out-of-order\n",
    "    \n",
    "     ### Initialize agent_sum_reward to zero in the form of a numpy array \n",
    "    # with shape (number of values for tau, number of step-sizes, number of runs, number of episodes)\n",
    "    agent_sum_reward = np.zeros((len(agent_parameters[\"tau\"]), len(agent_parameters[\"step_size\"]), experiment_parameters[\"num_runs\"], experiment_parameters[\"num_episodes\"]))\n",
    "    \n",
    "    ### Replace the Nones with the correct values in the rest of the code\n",
    "\n",
    "    # for loop over different values of tau\n",
    "    # tqdm is used to show a progress bar for completing the parameter study\n",
    "    for i in tqdm(range(len(agent_parameters[\"tau\"]))):\n",
    "    \n",
    "        # for loop over different values of the step-size\n",
    "        for j in range(len(agent_parameters[\"step_size\"])): \n",
    "\n",
    "            ### Specify env_info \n",
    "            env_info = {}\n",
    "\n",
    "            ### Specify agent_info\n",
    "            agent_info = {\"num_actions\": environment_parameters[\"num_actions\"],\n",
    "                          \"input_dim\": environment_parameters[\"input_dim\"],\n",
    "                          \"discount_factor\": environment_parameters[\"discount_factor\"],\n",
    "                          \"tau\": agent_parameters[\"tau\"][i],\n",
    "                          \"step_size\": agent_parameters[\"step_size\"][j]}\n",
    "\n",
    "            # for loop over runs\n",
    "            for run in range(experiment_parameters[\"num_runs\"]): \n",
    "                \n",
    "                # Set the seed\n",
    "                agent_info[\"seed\"] = agent_parameters[\"seed\"] * experiment_parameters[\"num_runs\"] + run\n",
    "                \n",
    "                # Beginning of the run            \n",
    "                rl_glue.rl_init(agent_info, env_info)\n",
    "\n",
    "                for episode in range(experiment_parameters[\"num_episodes\"]): \n",
    "                    \n",
    "                    # Run episode\n",
    "                    rl_glue.rl_episode(0) # no step limit\n",
    "\n",
    "                    ### Store sum of reward\n",
    "                    agent_sum_reward[i, j, run, episode] = rl_glue.rl_agent_message(\"get_sum_reward\")\n",
    "\n",
    "            if not os.path.exists('results'):\n",
    "                    os.makedirs('results')\n",
    "\n",
    "            save_name = \"{}\".format(rl_glue.agent.name).replace('.','')\n",
    "\n",
    "            # save sum reward\n",
    "            np.save(\"results/sum_reward_{}\".format(save_name), agent_sum_reward)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "df5e20c532d780d5353f044cefabda89",
     "grade": false,
     "grade_id": "cell-bca56669ed9bbce9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Execute the following code to test the implementation of `run_experiment()` with a dummy agent and a dummy environment. The test will cover 100 runs, 100 episodes, 12 step-size values, and 4 values for $\\tau$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4a6868062d024122787399fe9029e494",
     "grade": true,
     "grade_id": "cell-56541717f4f15d20",
     "locked": true,
     "points": 100,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:11<00:00,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed the assert!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Experiment parameters\n",
    "experiment_parameters = {\n",
    "    \"num_runs\" : 100,\n",
    "    \"num_episodes\" : 100,\n",
    "}\n",
    "\n",
    "# Environment parameters\n",
    "environment_parameters = {\n",
    "    \"input_dim\" : 8,\n",
    "    \"num_actions\": 4, \n",
    "    \"discount_factor\" : 0.99\n",
    "}\n",
    "\n",
    "agent_parameters = {\n",
    "    \"step_size\": 3e-5 * np.power(2.0, np.array([-6, -5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5])),\n",
    "    \"tau\": np.array([0.001, 0.01, 0.1, 1.0]),\n",
    "    \"seed\": 0\n",
    "}\n",
    "\n",
    "test_env = DummyEnvironment\n",
    "test_agent = DummyAgent\n",
    "\n",
    "run_experiment(test_env, \n",
    "               test_agent, \n",
    "               environment_parameters, \n",
    "               agent_parameters, \n",
    "               experiment_parameters)\n",
    "\n",
    "sum_reward_dummy_agent = np.load(\"results/sum_reward_dummy_agent.npy\")\n",
    "sum_reward_dummy_agent_answer = np.load(\"asserts/sum_reward_dummy_agent.npy\")\n",
    "assert(np.allclose(sum_reward_dummy_agent, sum_reward_dummy_agent_answer))\n",
    "\n",
    "print(\"Passed the assert!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "00c09538f65dff6afe75f359870f5126",
     "grade": false,
     "grade_id": "cell-bea11a6380a836e3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## 2. Parameter Study for the Neural Network Agent with Adam Optimizer\n",
    "\n",
    "With `run_experiment()` implemented for a dummy agent, the next step is to evaluate the performance of the agent developed in [Lunar Lander Agent](TODO) with various step-size parameters. This analysis will use parameter sensitivity curves, where the y-axis represents performance metrics and the x-axis shows the tested parameter values. Performance will be measured as the average return over episodes, averaged across 30 runs.\n",
    "\n",
    "In [Lunar Lander Agent](TODO), a step-size of $10^{-3}$ was used, yielding reasonable results. To explore other step-sizes, we can vary this value by multiplying it with powers of two:\n",
    "\n",
    "$10^{-3} \\times 2^x$ where $x \\in \\{-9, -8, -7, -6, -5, -4, -3, -2, -1, 0, 1, 2, 3\\}$\n",
    "\n",
    "\n",
    "Using powers of two allows for finer increments at smaller values and larger jumps at higher values.\n",
    "\n",
    "The results for this set of step-sizes are shown below:\n",
    "\n",
    "<img src=\"parameter_study.png\" alt=\"Parameter Study\" style=\"width: 500px;\"/>\n",
    "\n",
    "The optimal performance is observed for step-sizes in the range $[10^{-4}, 10^{-3}]$. Performance tends to decline for both larger and smaller step-size values. This narrow range where the agent performs well indicates that selecting an appropriate step-size is crucial.\n",
    "\n",
    "For performance assessment, the average return over episodes, averaged over 30 runs, was used. However, to analyze the impact of the step-size on the agent’s early or final performance, alternative metrics might be considered. For instance, to examine early performance, one could use the average return over the first 100 episodes, averaged over 30 runs. Adjusting the performance metric can provide different insights into how the step-size parameter affects the agent’s behavior."
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "complete-reinforcement-learning-system",
   "graded_item_id": "h4ZLq",
   "launcher_item_id": "rbt6a"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
