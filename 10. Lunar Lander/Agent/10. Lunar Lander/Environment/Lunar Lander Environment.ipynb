{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "91bab923ff51b8c4cc2db62df96dceba",
     "grade": false,
     "grade_id": "cell-728287ea719cc025",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Lunar Lander Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "75370e1d01e0e2216d0982ef3c22eb0a",
     "grade": false,
     "grade_id": "cell-e86bb7c59ff0a4a5",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Adapted from the Reinforcement Learning specialization course on Coursera, This challenging project involves creating a realistic simulator, selecting an appropriate reinforcement learning algorithm, implementing the algorithm, and optimizing its hyperparameters. This notebook will begin by taking the first steps in developing a lunar lander environment, a realistic lunar landing simulator suitable for training an agent for real-world deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c4a738c811626904fa17a90cbfddd80e",
     "grade": false,
     "grade_id": "cell-62c5c402edcd8ae5",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Creating an Environment\n",
    "\n",
    "The essential functions to facilitate the development of the lunar lander environment are:\n",
    "\n",
    "- **get_velocity**: Returns an array representing the x and y velocities of the lander, each within the range $[0, 60]$.\n",
    "- **get_angle**: Returns a scalar representing the angle of the lander, ranging from $[0, 359]$ degrees.\n",
    "- **get_position**: Returns an array with the x and y positions of the lander, each ranging from $[0, 100]$.\n",
    "- **get_landing_zone**: Returns an array with the x and y coordinates of the landing zone, each ranging from $[1, 100]$.\n",
    "- **get_fuel**: Returns a scalar indicating the remaining amount of fuel, starting at $100$ and within the range $[0, 100]$.\n",
    "\n",
    "These functions are provided as placeholders for this notebook.\n",
    "\n",
    "![Lunar Landar](lunar_landar.png)\n",
    "\n",
    "In this notebook, the provided functions will be used to **structure the reward signal** based on the following criteria:\n",
    "\n",
    "1. **Crash Condition**: The lander will crash if it touches the ground with a ``y_velocity < -3`` (downward velocity greater than three).\n",
    "\n",
    "2. **Crash Condition**: The lander will crash if it touches the ground with an ``x_velocity < -10`` or ``x_velocity > 10`` (horizontal speed exceeding 10).\n",
    "\n",
    "3. **Crash Condition**: The lander will crash if it touches the ground and the angle is such that ``5 < angle < 355`` (the angle deviates more than 5 degrees from vertical).\n",
    "\n",
    "4. **Crash Condition**: The lander will crash if it has not yet landed and ``fuel <= 0`` (the fuel runs out).\n",
    "\n",
    "5. **Fuel Efficiency**: Minimizing fuel consumption is preferred, as MST aims to save money on fuel.\n",
    "\n",
    "6. **Landing Zone**: The lander must land within the designated landing zone. It will crash if it touches the ground and the ``x_position`` is not within the ``landing_zone`` (the lander lands outside the designated zone).\n",
    "\n",
    "Complete the methods below to develop the lunar lander environment based on these criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "e3ee2fb8f0c76c238531ec8c61e9f799",
     "grade": false,
     "grade_id": "cell-b5475cc072c387ff",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "import environment\n",
    "import numpy as np\n",
    "from utils import get_landing_zone, get_angle, get_velocity, get_position, get_fuel, tests\n",
    "get_landing_zone()\n",
    "# Lunar Lander Environment\n",
    "class LunarLanderEnvironment(environment.BaseEnvironment):\n",
    "    def __init__(self):\n",
    "        self.current_state = None\n",
    "        self.count = 0\n",
    "    \n",
    "    def env_init(self, env_info):\n",
    "        # users set this up\n",
    "        self.state = np.zeros(6) # velocity x, y, angle, distance to ground, landing zone x, y\n",
    "    \n",
    "    def env_start(self):\n",
    "        land_x, land_y = get_landing_zone() # gets the x, y coordinate of the landing zone\n",
    "        # At the start we initialize the agent to the top left hand corner (100, 20) with 0 velocity \n",
    "        # in either any direction. The agent's angle is set to 0 and the landing zone is retrieved and set.\n",
    "        # The lander starts with fuel of 100.\n",
    "        # (vel_x, vel_y, angle, pos_x, pos_y, land_x, land_y, fuel)\n",
    "        self.current_state = (0, 0, 0, 100, 20, land_x, land_y, 100)\n",
    "        return self.current_state\n",
    "    \n",
    "    def env_step(self, action):\n",
    "        \n",
    "        land_x, land_y = get_landing_zone() # gets the x, y coordinate of the landing zone\n",
    "        vel_x, vel_y = get_velocity(action) # gets the x, y velocity of the lander\n",
    "        angle = get_angle(action) # gets the angle the lander is positioned in\n",
    "        pos_x, pos_y = get_position(action) # gets the x, y position of the lander\n",
    "        fuel = get_fuel(action) # get the amount of fuel remaining for the lander\n",
    "        \n",
    "        terminal = False\n",
    "        reward = 0.0\n",
    "        observation = (vel_x, vel_y, angle, pos_x, pos_y, land_x, land_y, fuel)\n",
    "        \n",
    "        # use the above observations to decide what the reward will be, and if the\n",
    "        # agent is in a terminal state.\n",
    "        # Recall - if the agent crashes or lands terminal needs to be set to True\n",
    "        \n",
    "        if pos_y <= land_y:\n",
    "            terminal = True\n",
    "            if (vel_y < -3) or (vel_x < -10 or vel_x > 10) or (5 < angle < 355) or (pos_x != land_x):\n",
    "                reward = -10000\n",
    "            else:\n",
    "                reward = fuel\n",
    "        elif fuel <= 0:\n",
    "            terminal = True\n",
    "            reward = -10000\n",
    "        \n",
    "        self.reward_obs_term = (reward, observation, terminal)\n",
    "        return self.reward_obs_term\n",
    "    \n",
    "    def env_cleanup(self):\n",
    "        return None\n",
    "    \n",
    "    def env_message(self):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "6f960ff843630e06b23aa258113d6e6b",
     "grade": false,
     "grade_id": "cell-9c57ff0d2b96ac51",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Evaluating the Reward Function\n",
    "\n",
    "Designing an optimal reward function can be complex, and defining what constitutes the \"best\" reward function is often ambiguous. Instead of relying on quantitative metrics, evaluate the reward function qualitatively. We provide a series of test cases below, illustrating transitions and explaining the behavior of a sample reward function. \n",
    "\n",
    "As you review these cases, compare them with how your own reward function performs and assess whether it behaves as expected. Although the final stages of the capstone will use a standardized lunar lander environment implementation, this notebook focuses on gaining experience with environment and reward function design."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "8ea30a25d9a5f478e892745463ddacbc",
     "grade": false,
     "grade_id": "cell-2d30dbf6446d5afc",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Case 1: Uncertain Future\n",
    "\n",
    "In this scenario, the lander is located in the top left corner of the screen, moving with a velocity of (12, 15) and has 10 units of fuel. The outcome of this landing attempt is still uncertain.\n",
    "\n",
    "![Lunar Landar](lunar_landar_1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "df58fb55b3df83a3a12e6a16a5016887",
     "grade": false,
     "grade_id": "cell-99abd81376335339",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward: 0.0, Terminal: False\n"
     ]
    }
   ],
   "source": [
    "tests(LunarLanderEnvironment, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ca2d70140549a4b7eb3c52787154953b",
     "grade": false,
     "grade_id": "cell-89911113f764c447",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "In this scenario, the agent did not receive any reward since it neither accomplished the objective nor experienced a crash. One approach could be to assign a positive reward for progress toward the goal, or a negative reward for fuel consumption. How does your reward function handle this situation?\n",
    "\n",
    "Additionally, verify that the `Terminal` status is set to `False`. The agent has not yet landed, crashed, or depleted its fuel, so the episode is still ongoing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f6c45e927b165cdb95744caaf3309aed",
     "grade": false,
     "grade_id": "cell-b19c487e9da05800",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Case 2: Imminent Crash!\n",
    "\n",
    "The lander is situated in the target landing zone at a 45-degree angle, but its landing gear is designed to handle only a five-degree angular offset. As a result, it is on the verge of crashing!\n",
    "\n",
    "![Lunar Landar](lunar_landar_2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d16b6f3b9c7415830ef1b096fd5a36f7",
     "grade": false,
     "grade_id": "cell-9b3900153803f78e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward: -10000, Terminal: True\n"
     ]
    }
   ],
   "source": [
    "tests(LunarLanderEnvironment, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "48870c2ad1832288abe07ea16cc88ba2",
     "grade": false,
     "grade_id": "cell-4731b02a8f54214b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "A reward of -10,000 was assigned to penalize the agent for crashing.\n",
    "\n",
    "The `Terminal` status is set to `True` since the agent has crashed and the episode has concluded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a06bb6be22b15cb43cad3378583b188c",
     "grade": false,
     "grade_id": "cell-af000f1895c6bd69",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Case 3: Perfect Landing!\n",
    "\n",
    "The lander is upright and situated within the target landing zone, with five units of fuel remaining. The landing is executed successfully!\n",
    "\n",
    "![Lunar Landar](lunar_landar_3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "dd0b04ee97dbc1ba8267d05a81c6f393",
     "grade": false,
     "grade_id": "cell-6a53769313d85b0b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward: 5, Terminal: True\n"
     ]
    }
   ],
   "source": [
    "tests(LunarLanderEnvironment, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7fcd0832a9a99dd4d6ae3cd1f2a99666",
     "grade": false,
     "grade_id": "cell-e23d284c1e7c6865",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "To encourage the agent to conserve as much fuel as possible, we reward successful landings proportionally to the amount of fuel remaining. Here, we gave the agent a reward of five since it landed with five units of fuel remaining. How did you incentivize the agent to be fuel efficient?\n",
    "\n",
    "The `Terminal` status is set to `True` since the agent has landed and the episode is over."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "144e1370f8a1146808c367d077c8a6db",
     "grade": false,
     "grade_id": "cell-21cc28d788b4455d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Case 4: Fuel Depletion Alert!\n",
    "\n",
    "The lander is directly above the target landing zone but has depleted its fuel supply. The agent faces a critical situationâ€”without fuel, it cannot avoid a crash!\n",
    "\n",
    "![Lunar Landar](lunar_landar_4.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "dff5eca159850a13f9e20aedb800fa8c",
     "grade": false,
     "grade_id": "cell-86ece2998b73491a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward: -10000, Terminal: True\n"
     ]
    }
   ],
   "source": [
    "tests(LunarLanderEnvironment, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "14711bd7c29c70cfc7f2779a4a4548ff",
     "grade": false,
     "grade_id": "cell-e1cc048da1ecc920",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "We gave the agent a reward of -10000 to punish it for crashing.\n",
    "As before, ``Terminal`` is set to ``True`` since the agent has crashed and the episode is over."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 5: Where's The Landing Zone?!\n",
    "\n",
    "The lander is touching down at a vertical angle with fuel to spare. But it is not in the landing zone and the surface is uneven &mdash; it is going to crash!\n",
    "\n",
    "![Lunar Landar](lunar_landar_5.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward: -10000, Terminal: True\n"
     ]
    }
   ],
   "source": [
    "tests(LunarLanderEnvironment, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We gave the agent a reward of -10000 to punish it for crashing.\n",
    "``Terminal`` is set to ``True`` since the agent has crashed and the episode is over."
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
