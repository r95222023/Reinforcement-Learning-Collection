# Reinforcement Learning Collection

This repository provides implementations of essential reinforcement learning algorithms, adapted from the Reinforcement Learning specialization course on Coursera. It aims to deliver a comprehensive collection of well-documented and user-friendly models for educational and research purposes. Each model includes detailed explanations and code examples to facilitate understanding and practical application.

Notebook | Description
:- | :-
[Bandits and Exploration-Exploitation.ipynb](https://github.com/r95222023/Reinforcement-Learning-Collection/blob/main/1.%20Bandits%20and%20Exploration-Exploitation/Bandits%20and%20Exploration-Exploitation.ipynb) | Build a bandit-solving agent and demonstrate the impact of epsilon on exploration and the exploration/exploitation tradeoff.
[Optimal Policies with Dynamic Programming.ipynb](https://github.com/r95222023/Reinforcement-Learning-Collection/blob/main/2.%20Optimal%20Policies%20with%20Dynamic%20Programming/Optimal%20Policies%20with%20Dynamic%20Programming.ipynb) | Create a gridworld city-solving agent that illustrates Policy Evaluation and Policy Improvement, Value and Policy Iteration, Bellman Equations, and Synchronous and Asynchronous Methods.
[Policy Evaluation in Cliff Walking Environment.ipynb](https://github.com/r95222023/Reinforcement-Learning-Collection/blob/main/3.%20Policy%20Evaluation%20with%20Temporal%20Difference%20Learning/Policy%20Evaluation%20in%20Cliff%20Walking%20Environment.ipynb) | Implement a sample-based, bootstrapping, model-free reinforcement learning agent for prediction, specifically using one-step temporal difference learning (TD(0)). Design an agent for policy evaluation in the Cliff Walking environment to accurately estimate state values given a specific policy.
[Q-Learning and Expected Sarsa.ipynb](https://github.com/r95222023/Reinforcement-Learning-Collection/blob/main/4.%20Q-Learning%20and%20Expected%20Sarsa/Q-Learning%20and%20Expected%20Sarsa.ipynb) | Implement Q-Learning and Expected Sarsa with $\epsilon$-greedy action selection. Investigate the behavior of these two algorithms in the Cliff World environment.
[Dyna-Q and Dyna-Q+.ipynb](https://github.com/r95222023/Reinforcement-Learning-Collection/blob/main/5.%20Dyna-Q%20and%20Dyna-Q%2B/Dyna-Q%20and%20Dyna-Q%2B.ipynb) |  Implement Dyna-Q and Dyna-Q+ algorithms. Explore the behavior of these two algorithms in the Shortcut Maze environment.
[TD with State Aggregation.ipynb](https://github.com/r95222023/Reinforcement-Learning-Collection/blob/main/6.%20Semi-gradient%20TD(0)%20with%20State%20Aggregation/TD%20with%20State%20Aggregation.ipynb) | Implement semi-gradient TD(0) with function approximation (state aggregation) in a 500-state random walk environment. Demonstrate the use of supervised learning approaches to approximate value functions and compare the impact of different resolutions of state aggregation.
[Semi-gradient TD with a Neural Network.ipynb](https://github.com/r95222023/Reinforcement-Learning-Collection/blob/main/7.%20Semi-gradient%20TD%20with%20a%20Neural%20Network/Semi-gradient%20TD%20with%20a%20Neural%20Network.ipynb) |Implement semi-gradient TD(0) with a neural network in a 500-state random walk environment. The rest of analyzation follows "TD with State Aggregation"
[Function Approximation and Control.ipynb](https://github.com/r95222023/Reinforcement-Learning-Collection/blob/main/8.%20Function%20Approximation%20and%20Control/Function%20Approximation%20and%20Control.ipynb) | Apply function approximation in a control setting, implement the Sarsa algorithm with tile coding, and compare three tile coding configurations to assess their impact on the agent.
[Average Reward Softmax Actor-Critic.ipynb](https://github.com/r95222023/Reinforcement-Learning-Collection/blob/main/9.%20Average%20Reward%20Softmax%20Actor-Critic%20using%20Tile-coding/Average%20Reward%20Softmax%20Actor-Critic.ipynb) | Implement the Average Reward Softmax Actor-Critic algorithm for the Pendulum Swing-Up problem. Learn how to parameterize the policy as a function in a discrete action environment.
[Lunar Lander Environment.ipynb](https://github.com/r95222023/Reinforcement-Learning-Collection/blob/main/10.%20Lunar%20Lander/Environment/Lunar%20Lander%20Environment.ipynb) | Take the first steps in developing a lunar lander environment, a realistic lunar landing simulator suitable for training an agent for real-world deployment.
[Lunar Lander Agent.ipynb](https://github.com/r95222023/Reinforcement-Learning-Collection/blob/main/10.%20Lunar%20Lander/Agent/Lunar%20Lander%20Agent.ipynb) | Build an Expected Sarsa agent to solve the Lunar Lander problem using the environment. The agent utilizes Adam optimization and replay buffers.
[Parameter Study for Lunar Lander.ipynb](https://github.com/r95222023/Reinforcement-Learning-Collection/blob/main/10.%20Lunar%20Lander/Parameter%20Study/Parameter%20Study%20for%20Lunar%20Lander.ipynb) | Analyze agent performance across various parameters including step-size, the temperature parameter for the softmax policy, and the replay buffer capacity through careful experimentation.